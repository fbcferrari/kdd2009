{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando as libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import (auc,roc_curve, recall_score, classification_report\n",
    "                             precision_recall_fscore_support,roc_auc_score)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_SEED=333 #metade do número da besta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando o arquivo principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros em dados_X: 50000\n",
      "Registros em churn_labels: 50000 \n",
      "Registros em appetency_labels: 50000\n",
      "Registros em upselling_labels: 50000\n"
     ]
    }
   ],
   "source": [
    "dados_X = pd.read_csv('dados/orange_small_train.data',sep='\\t')\n",
    "#Headers = None pois a primeira linha já é registro válido\n",
    "churn_labels = pd.read_csv('dados/labels/orange_small_train_churn.labels',sep='\\t', header=None)\n",
    "appetency_labels = pd.read_csv('dados/labels/orange_small_train_appetency.labels',sep='\\t', header=None)\n",
    "upselling_labels = pd.read_csv('dados/labels/orange_small_train_upselling.labels',sep='\\t', header=None)\n",
    "print(f'Registros em dados_X: {dados_X.shape[0]}\\nRegistros em churn_labels: {churn_labels.shape[0]} \\\n",
    "\\nRegistros em appetency_labels: {appetency_labels.shape[0]}\\nRegistros em upselling_labels: {upselling_labels.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>...</th>\n",
       "      <th>Var180</th>\n",
       "      <th>Var181</th>\n",
       "      <th>Var182</th>\n",
       "      <th>Var183</th>\n",
       "      <th>Var184</th>\n",
       "      <th>Var185</th>\n",
       "      <th>Var186</th>\n",
       "      <th>Var187</th>\n",
       "      <th>Var188</th>\n",
       "      <th>Var189</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>5.000000e+04</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.487179</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>425.298387</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>2.387933e+05</td>\n",
       "      <td>1326.437116</td>\n",
       "      <td>6.809496</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.145299</td>\n",
       "      <td>3.926057e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>3.776755e+06</td>\n",
       "      <td>0.611456</td>\n",
       "      <td>1.416638e+06</td>\n",
       "      <td>7.777380e+04</td>\n",
       "      <td>8.460919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.299145</td>\n",
       "      <td>16.544160</td>\n",
       "      <td>167.368477</td>\n",
       "      <td>270.142137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.820354</td>\n",
       "      <td>0.022352</td>\n",
       "      <td>672.206258</td>\n",
       "      <td>0.226593</td>\n",
       "      <td>1.110451e+05</td>\n",
       "      <td>2532.849475</td>\n",
       "      <td>5.965363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.326822</td>\n",
       "      <td>1.599995e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>4.482539e+05</td>\n",
       "      <td>2.367372</td>\n",
       "      <td>4.050112e+05</td>\n",
       "      <td>3.175127e+04</td>\n",
       "      <td>7.397512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.039849</td>\n",
       "      <td>7.130844</td>\n",
       "      <td>17.949779</td>\n",
       "      <td>56.221658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.420000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.487179</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>425.298387</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>2.387933e+05</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.145299</td>\n",
       "      <td>3.926057e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>3.776755e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.416638e+06</td>\n",
       "      <td>7.777380e+04</td>\n",
       "      <td>8.460919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.299145</td>\n",
       "      <td>16.544160</td>\n",
       "      <td>167.368477</td>\n",
       "      <td>270.142137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11.487179</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>425.298387</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>2.387933e+05</td>\n",
       "      <td>945.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.145299</td>\n",
       "      <td>3.926057e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>3.776755e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.416638e+06</td>\n",
       "      <td>7.777380e+04</td>\n",
       "      <td>8.460919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.299145</td>\n",
       "      <td>16.544160</td>\n",
       "      <td>167.368477</td>\n",
       "      <td>270.142137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.487179</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>425.298387</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>2.387933e+05</td>\n",
       "      <td>1326.437116</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.145299</td>\n",
       "      <td>3.926057e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>3.776755e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.416638e+06</td>\n",
       "      <td>7.777380e+04</td>\n",
       "      <td>8.460919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.299145</td>\n",
       "      <td>16.544160</td>\n",
       "      <td>167.368477</td>\n",
       "      <td>270.142137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>680.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>130668.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>6.048550e+06</td>\n",
       "      <td>131761.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2300.000000</td>\n",
       "      <td>1.232559e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.428483e+07</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.199478e+07</td>\n",
       "      <td>3.048400e+06</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>910.000000</td>\n",
       "      <td>628.620000</td>\n",
       "      <td>642.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Var1          Var2           Var3          Var4          Var5  \\\n",
       "count  50000.000000  50000.000000   50000.000000  50000.000000  5.000000e+04   \n",
       "mean      11.487179      0.004029     425.298387      0.125396  2.387933e+05   \n",
       "std        4.820354      0.022352     672.206258      0.226593  1.110451e+05   \n",
       "min        0.000000      0.000000       0.000000      0.000000  0.000000e+00   \n",
       "25%       11.487179      0.004029     425.298387      0.125396  2.387933e+05   \n",
       "50%       11.487179      0.004029     425.298387      0.125396  2.387933e+05   \n",
       "75%       11.487179      0.004029     425.298387      0.125396  2.387933e+05   \n",
       "max      680.000000      5.000000  130668.000000     27.000000  6.048550e+06   \n",
       "\n",
       "                Var6          Var7     Var8          Var9         Var10  ...  \\\n",
       "count   50000.000000  50000.000000  50000.0  50000.000000  5.000000e+04  ...   \n",
       "mean     1326.437116      6.809496      0.0     48.145299  3.926057e+05  ...   \n",
       "std      2532.849475      5.965363      0.0     18.326822  1.599995e+05  ...   \n",
       "min         0.000000      0.000000      0.0      0.000000  0.000000e+00  ...   \n",
       "25%       581.000000      0.000000      0.0     48.145299  3.926057e+05  ...   \n",
       "50%       945.000000      7.000000      0.0     48.145299  3.926057e+05  ...   \n",
       "75%      1326.437116      7.000000      0.0     48.145299  3.926057e+05  ...   \n",
       "max    131761.000000    140.000000      0.0   2300.000000  1.232559e+07  ...   \n",
       "\n",
       "             Var180        Var181        Var182        Var183        Var184  \\\n",
       "count  5.000000e+04  50000.000000  5.000000e+04  5.000000e+04  50000.000000   \n",
       "mean   3.776755e+06      0.611456  1.416638e+06  7.777380e+04      8.460919   \n",
       "std    4.482539e+05      2.367372  4.050112e+05  3.175127e+04      7.397512   \n",
       "min    0.000000e+00      0.000000  0.000000e+00  0.000000e+00      0.000000   \n",
       "25%    3.776755e+06      0.000000  1.416638e+06  7.777380e+04      8.460919   \n",
       "50%    3.776755e+06      0.000000  1.416638e+06  7.777380e+04      8.460919   \n",
       "75%    3.776755e+06      0.000000  1.416638e+06  7.777380e+04      8.460919   \n",
       "max    1.428483e+07     49.000000  1.199478e+07  3.048400e+06   1200.000000   \n",
       "\n",
       "        Var185        Var186        Var187        Var188        Var189  \n",
       "count  50000.0  50000.000000  50000.000000  50000.000000  50000.000000  \n",
       "mean       0.0      3.299145     16.544160    167.368477    270.142137  \n",
       "std        0.0      1.039849      7.130844     17.949779     56.221658  \n",
       "min        0.0      0.000000      0.000000     -6.420000      6.000000  \n",
       "25%        0.0      3.299145     16.544160    167.368477    270.142137  \n",
       "50%        0.0      3.299145     16.544160    167.368477    270.142137  \n",
       "75%        0.0      3.299145     16.544160    167.368477    270.142137  \n",
       "max        0.0    102.000000    910.000000    628.620000    642.000000  \n",
       "\n",
       "[8 rows x 189 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dados_X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformando o sim e não dos labels (estão -1 e 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    46328\n",
       " 1     3672\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_labels[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    46328\n",
       "1     3672\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn = churn_labels.ix[:, 0].astype('category')\n",
    "churn.cat.rename_categories([0, 1], inplace=True)\n",
    "appetency = appetency_labels.ix[:, 0].astype('category')\n",
    "appetency.cat.rename_categories([0, 1], inplace=True)\n",
    "upselling = upselling_labels.ix[:, 0].astype('category')\n",
    "upselling.cat.rename_categories([0, 1], inplace=True)\n",
    "churn.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proporção de 0 e 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'churn')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2160x720 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dados_plotar = pd.concat([churn,appetency,upselling], axis=1)\n",
    "dados_plotar.columns = ['churn','appetency','upselling']\n",
    "fig = plt.figure(figsize=(30, 10))\n",
    "\n",
    "plt.subplot(131)\n",
    "ax0 = dados_plotar['upselling'].hist(bins=4)\n",
    "ax0.set_ylabel('upselling')\n",
    "\n",
    "plt.subplot(132)\n",
    "ax1 = dados_plotar['appetency'].hist(bins=4)\n",
    "ax1.set_ylabel('appetency')\n",
    "\n",
    "plt.subplot(133)\n",
    "ax2= dados_plotar['churn'].hist(bins=4)\n",
    "ax2.set_ylabel('churn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Gap entre os valores é muito alto, será realizado um resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratamento das colunas category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>...</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>702.000000</td>\n",
       "      <td>1241.000000</td>\n",
       "      <td>1240.000000</td>\n",
       "      <td>1579.000000</td>\n",
       "      <td>1.487000e+03</td>\n",
       "      <td>44471.000000</td>\n",
       "      <td>44461.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>702.000000</td>\n",
       "      <td>1.487000e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>50000.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.487179</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>425.298387</td>\n",
       "      <td>0.125396</td>\n",
       "      <td>2.387933e+05</td>\n",
       "      <td>1326.437116</td>\n",
       "      <td>6.809496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.145299</td>\n",
       "      <td>3.926057e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>3.984080</td>\n",
       "      <td>2123.702060</td>\n",
       "      <td>0.302480</td>\n",
       "      <td>-0.983600</td>\n",
       "      <td>-0.220100</td>\n",
       "      <td>10.115720</td>\n",
       "      <td>2.103060</td>\n",
       "      <td>10.233400</td>\n",
       "      <td>-0.368800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>40.709951</td>\n",
       "      <td>0.141933</td>\n",
       "      <td>4270.193518</td>\n",
       "      <td>1.275481</td>\n",
       "      <td>6.441259e+05</td>\n",
       "      <td>2685.693668</td>\n",
       "      <td>6.326053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>154.777855</td>\n",
       "      <td>9.280896e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.119166</td>\n",
       "      <td>1183.541455</td>\n",
       "      <td>1.070238</td>\n",
       "      <td>0.127009</td>\n",
       "      <td>0.933506</td>\n",
       "      <td>5.798438</td>\n",
       "      <td>0.867994</td>\n",
       "      <td>6.488254</td>\n",
       "      <td>0.799074</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>518.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1066.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>861.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2230.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.187425e+05</td>\n",
       "      <td>1428.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>2.628630e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2984.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>680.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>130668.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>6.048550e+06</td>\n",
       "      <td>131761.000000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2300.000000</td>\n",
       "      <td>1.232559e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4290.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Var1         Var2           Var3         Var4          Var5  \\\n",
       "count  702.000000  1241.000000    1240.000000  1579.000000  1.487000e+03   \n",
       "mean    11.487179     0.004029     425.298387     0.125396  2.387933e+05   \n",
       "std     40.709951     0.141933    4270.193518     1.275481  6.441259e+05   \n",
       "min      0.000000     0.000000       0.000000     0.000000  0.000000e+00   \n",
       "25%      0.000000     0.000000       0.000000     0.000000  0.000000e+00   \n",
       "50%      0.000000     0.000000       0.000000     0.000000  0.000000e+00   \n",
       "75%     16.000000     0.000000       0.000000     0.000000  1.187425e+05   \n",
       "max    680.000000     5.000000  130668.000000    27.000000  6.048550e+06   \n",
       "\n",
       "                Var6          Var7  Var8         Var9         Var10  ...  \\\n",
       "count   44471.000000  44461.000000   0.0   702.000000  1.487000e+03  ...   \n",
       "mean     1326.437116      6.809496   NaN    48.145299  3.926057e+05  ...   \n",
       "std      2685.693668      6.326053   NaN   154.777855  9.280896e+05  ...   \n",
       "min         0.000000      0.000000   NaN     0.000000  0.000000e+00  ...   \n",
       "25%       518.000000      0.000000   NaN     4.000000  0.000000e+00  ...   \n",
       "50%       861.000000      7.000000   NaN    20.000000  0.000000e+00  ...   \n",
       "75%      1428.000000      7.000000   NaN    46.000000  2.628630e+05  ...   \n",
       "max    131761.000000    140.000000   NaN  2300.000000  1.232559e+07  ...   \n",
       "\n",
       "             Var221        Var222        Var223        Var224        Var225  \\\n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000  50000.000000   \n",
       "mean       3.984080   2123.702060      0.302480     -0.983600     -0.220100   \n",
       "std        1.119166   1183.541455      1.070238      0.127009      0.933506   \n",
       "min        0.000000      0.000000     -1.000000     -1.000000     -1.000000   \n",
       "25%        4.000000   1066.000000      0.000000     -1.000000     -1.000000   \n",
       "50%        4.000000   2230.000000      0.000000     -1.000000     -1.000000   \n",
       "75%        4.000000   2984.250000      0.000000     -1.000000      1.000000   \n",
       "max        6.000000   4290.000000      3.000000      0.000000      2.000000   \n",
       "\n",
       "             Var226        Var227        Var228        Var229  Var230  \n",
       "count  50000.000000  50000.000000  50000.000000  50000.000000     0.0  \n",
       "mean      10.115720      2.103060     10.233400     -0.368800     NaN  \n",
       "std        5.798438      0.867994      6.488254      0.799074     NaN  \n",
       "min        0.000000      0.000000      0.000000     -1.000000     NaN  \n",
       "25%        6.000000      2.000000      8.000000     -1.000000     NaN  \n",
       "50%       10.000000      2.000000      8.000000     -1.000000     NaN  \n",
       "75%       14.000000      2.000000      8.000000      0.000000     NaN  \n",
       "max       22.000000      6.000000     29.000000      3.000000     NaN  \n",
       "\n",
       "[8 rows x 230 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colunas_objetos = dados_X.select_dtypes(include=['object']).columns\n",
    "dados_X[colunas_objetos] = dados_X[colunas_objetos].astype('category').apply(lambda x: x.cat.codes)\n",
    "dados_X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No enunciado da questão é avisado que as ultimas 40 colunas são categoricas, elas serão retiradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_drop = []\n",
    "\n",
    "for i in range(dados_X.shape[1]-40, dados_X.shape[1]+1):\n",
    "    value = 0\n",
    "    value = 'Var{}'.format(i)\n",
    "    var_drop.append(value)\n",
    "var_drop\n",
    "dados_X = dados_X.drop(var_drop,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Há dados com count=0, nulos, vou limpar antes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace dos nulos pela média\n",
    "dados_X = dados_X.fillna(dados_X.mean())\n",
    "dados_X = dados_X.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Será necessário realizar um resample, pois sem ele os meus resultados era proximo de ROC_AUX = 0.60.\n",
    "Tive a ideia neste link: https://chrisalbon.com/machine_learning/preprocessing_structured_data/handling_imbalanced_classes_with_upsampling/\n",
    "Mas fiquei com preconceito depois de ver que estava usando o Iris DataSet, então foi pra documentação do Sklearn\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.utils.resample.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Montando o DataFrame com os dados de treino e os resultados de churn\n",
    "churnDf = pd.DataFrame(churn)\n",
    "data = pd.concat([dados_X,churnDf],axis=1)\n",
    "data.columns=[np.append(dados_X.columns,'churn')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_neg = data[churn==0]\n",
    "data_pos = data[churn==1]\n",
    "\n",
    "data_pos_upsampled = resample(data_pos, \n",
    "                                replace=True,\n",
    "                                 n_samples=data_neg.shape[0],\n",
    "                                 random_state=RANDOM_SEED)\n",
    "\n",
    "df_upsampled = pd.concat([data_neg, data_pos_upsampled])\n",
    "df_X = df_upsampled.drop(['churn'],axis=1)\n",
    "df_y = df_upsampled['churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Será usado StratifiedKFold, segundo a sua documentação:\n",
    "Takes group information into account to avoid building folds with imbalanced class distributions (for binary or multiclass classification tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: AUC->0.9039291882556131 Profundidade->15.0 Estimators->400.0\n",
      "1 fold: AUC->0.8695487910189983 Profundidade->15.0 Estimators->200.0\n",
      "2 fold: AUC->0.8918393782383419 Profundidade->15.0 Estimators->500.0\n",
      "3 fold: AUC->0.9022994710137104 Profundidade->15.0 Estimators->200.0\n",
      "4 fold: AUC->0.856633919896362 Profundidade->15.0 Estimators->200.0\n",
      "AUC: 0.88+-0.02\n"
     ]
    }
   ],
   "source": [
    "x = df_X.loc[:]\n",
    "y = df_y.loc[:]\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "i =0\n",
    "aucs_rf_churn = []\n",
    "for train,test in cv.split(x,y):    \n",
    "    X_train = x.iloc[train]\n",
    "    X_test = x.iloc[test]\n",
    "    y_train = y.iloc[train]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    X_train = preprocessing.scale(X_train)\n",
    "    X_test = preprocessing.scale(X_test)\n",
    "    \n",
    "    resultados = np.empty([0, 3])\n",
    "    #Testando várias variações dos parametros e pegando o melhor\n",
    "    for n_estimators in range(50,550,50):\n",
    "        for max_depth in range(1,16):\n",
    "            rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, \n",
    "                                        class_weight='balanced',max_features ='sqrt',random_state = RANDOM_SEED)\n",
    "            pred = rf.fit(X_train,y_train).predict(X_test)\n",
    "            auc = roc_auc_score(y_test,pred)                \n",
    "            saida = np.array([n_estimators,max_depth, auc])            \n",
    "            resultados = np.vstack((resultados, saida))\n",
    "    n_estimators = resultados[np.argmax(resultados[:,2]),0]\n",
    "    max_depth = resultados[np.argmax(resultados[:,2]),1]\n",
    "    auc = resultados[np.argmax(resultados[:,2]),2]   \n",
    "    print(f'{i} fold: AUC->{auc} Profundidade->{max_depth} Estimators->{n_estimators}')\n",
    "    \n",
    "    #pred = rf.fit(X_train,y_train).predict(X_test)\n",
    "    aucs_rf_churn.append(auc)    \n",
    "    i=i+1\n",
    "\n",
    "AUC_rf_1 = '{}+-{}'.format(round(np.average(aucs_rf_churn), 2),round(np.std(aucs_rf_churn),2))\n",
    "print('AUC:', AUC_rf_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora será utilizado o MLPClassifier (também utilizado por mim quando trabalhava com IBM Watson Data Studio)\n",
    "Peguei um exemplo de seu uso em: https://www.kaggle.com/ahmethamzaemra/mlpclassifier-example/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold: AUC->0.9222965680984243\n",
      "2 fold: AUC->0.8799374055687459\n",
      "3 fold: AUC->0.885063673645586\n",
      "4 fold: AUC->0.9007555315704263\n",
      "5 fold: AUC->0.8744738262277388\n",
      "AUC: 0.89+-0.02\n"
     ]
    }
   ],
   "source": [
    "x = df_X.loc[:]\n",
    "y = df_y.loc[:]\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(256,128,64,32,16,8),learning_rate_init = 0.01, max_iter = 800)\n",
    "i =0\n",
    "aucs_mlp_churn = []\n",
    "for train,test in cv.split(x,y):    \n",
    "    X_train = x.iloc[train]\n",
    "    X_test = x.iloc[test]\n",
    "    y_train = y.iloc[train]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    X_train = preprocessing.scale(X_train)\n",
    "    X_test = preprocessing.scale(X_test)\n",
    "    \n",
    "    \n",
    "    pred = mlp.fit(X_train,y_train).predict(X_test)\n",
    "    aucs_mlp_churn.append(roc_auc_score(y_test, pred))\n",
    "    i=i+1\n",
    "    print(f'{i} fold: AUC->{roc_auc_score(y_test, pred)}')\n",
    "    \n",
    "\n",
    "AUC_mlp_1 = '{}+-{}'.format(round(np.average(aucs_mlp_churn), 2),round(np.std(aucs_mlp_churn),2))\n",
    "print('AUC:', AUC_mlp_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Houve 'empate tecnico' nos scores do dois algoritmos (0.91 com o desvio padrão otimista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsample agora para Appetency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Montando o DataFrame com os dados de treino e os resultados de appetency\n",
    "appetency_df = pd.DataFrame(appetency)\n",
    "data = pd.concat([dados_X,appetency_df],axis=1)\n",
    "data.columns=[np.append(dados_X.columns,'appetency')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neg = data[appetency==0]\n",
    "data_pos = data[appetency==1]\n",
    "\n",
    "data_pos_upsampled = resample(data_pos, \n",
    "                                replace=True,\n",
    "                                 n_samples=data_neg.shape[0],\n",
    "                                 random_state=RANDOM_SEED)\n",
    "\n",
    "df_upsampled = pd.concat([data_neg, data_pos_upsampled])\n",
    "df_X = df_upsampled.drop(['appetency'],axis=1)\n",
    "df_y = df_upsampled['appetency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: AUC->0.9013439218081858 Profundidade->14.0 Estimators->100.0\n",
      "1 fold: AUC->0.915444919568316 Profundidade->15.0 Estimators->250.0\n",
      "2 fold: AUC->0.9067399714925677 Profundidade->15.0 Estimators->500.0\n",
      "3 fold: AUC->0.9080126247200163 Profundidade->15.0 Estimators->250.0\n",
      "4 fold: AUC->0.9205355324781104 Profundidade->15.0 Estimators->100.0\n",
      "AUC: 0.91+-0.01\n"
     ]
    }
   ],
   "source": [
    "x = df_X.loc[:]\n",
    "y = df_y.loc[:]\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "i =0\n",
    "aucs_rf_appetency = []\n",
    "for train,test in cv.split(x,y):    \n",
    "    X_train = x.iloc[train]\n",
    "    X_test = x.iloc[test]\n",
    "    y_train = y.iloc[train]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    X_train = preprocessing.scale(X_train)\n",
    "    X_test = preprocessing.scale(X_test)\n",
    "    \n",
    "    resultados = np.empty([0, 3])\n",
    "    #Testando várias variações dos parametros e pegando o melhor\n",
    "    for n_estimators in range(50,550,50):\n",
    "        for max_depth in range(1,16):\n",
    "            rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, \n",
    "                                        class_weight='balanced',max_features ='sqrt',random_state = RANDOM_SEED)\n",
    "            pred = rf.fit(X_train,y_train).predict(X_test)\n",
    "            auc = roc_auc_score(y_test,pred)                \n",
    "            saida = np.array([n_estimators,max_depth, auc])            \n",
    "            resultados = np.vstack((resultados, saida))\n",
    "    n_estimators = resultados[np.argmax(resultados[:,2]),0]\n",
    "    max_depth = resultados[np.argmax(resultados[:,2]),1]\n",
    "    auc = resultados[np.argmax(resultados[:,2]),2]   \n",
    "    print(f'{i} fold: AUC->{auc} Profundidade->{max_depth} Estimators->{n_estimators}')    \n",
    "    \n",
    "    aucs_rf_appetency.append(auc)    \n",
    "    i=i+1    \n",
    "\n",
    "AUC_rf_2 = '{}+-{}'.format(round(np.average(aucs_rf_appetency), 2),round(np.std(aucs_rf_appetency),2))\n",
    "print('AUC:', AUC_rf_2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold: AUC->0.9147236614853195\n",
      "2 fold: AUC->0.9013385146804835\n",
      "3 fold: AUC->0.9305375647668392\n",
      "4 fold: AUC->0.8601964806218287\n",
      "5 fold: AUC->0.9415416171866565\n",
      "AUC: 0.91+-0.03\n"
     ]
    }
   ],
   "source": [
    "x = df_X.loc[:]\n",
    "y = df_y.loc[:]\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "#mlp = MLPClassifier(hidden_layer_sizes=(256,128,64,32,16,8),learning_rate_init = 0.01, max_iter = 800)\n",
    "i =0\n",
    "AUCs_mlp_2 = []\n",
    "for train,test in cv.split(x,y):    \n",
    "    X_train = x.iloc[train]\n",
    "    X_test = x.iloc[test]\n",
    "    y_train = y.iloc[train]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    X_train = preprocessing.scale(X_train)\n",
    "    X_test = preprocessing.scale(X_test)\n",
    "        \n",
    "    pred = mlp.fit(X_train,y_train).predict(X_test)\n",
    "    AUCs_mlp_2.append(roc_auc_score(y_test, pred))\n",
    "    i=i+1\n",
    "    print(f'{i} fold: AUC->{roc_auc_score(y_test, pred)}')\n",
    "\n",
    "AUC_mlp_2 = '{}+-{}'.format(round(np.average(AUCs_mlp_2), 2),round(np.std(AUCs_mlp_2),2))\n",
    "print('AUC:', AUC_mlp_2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melhor resultado utilizando o MLP (0.91+-0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upsample agora para upselling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Montando o DataFrame com os dados de treino e os resultados de upselling\n",
    "upselling_df = pd.DataFrame(upselling)\n",
    "data = pd.concat([dados_X,upselling_df],axis=1)\n",
    "data.columns=[np.append(dados_X.columns,'upselling')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neg = data[upselling==0]\n",
    "data_pos = data[upselling==1]\n",
    "\n",
    "data_pos_upsampled = resample(data_pos, \n",
    "                                replace=True, \n",
    "                                 n_samples=data_neg.shape[0],\n",
    "                                 random_state=RANDOM_SEED)\n",
    "\n",
    "df_upsampled = pd.concat([data_neg, data_pos_upsampled])\n",
    "df_X = df_upsampled.drop(['upselling'],axis=1)\n",
    "df_y = df_upsampled['upselling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fold: AUC->0.8805051813471502 Profundidade->15.0 Estimators->200.0\n",
      "1 fold: AUC->0.8943760794473229 Profundidade->15.0 Estimators->250.0\n",
      "2 fold: AUC->0.8813687392055268 Profundidade->15.0 Estimators->350.0\n",
      "3 fold: AUC->0.8471877361545935 Profundidade->15.0 Estimators->350.0\n",
      "4 fold: AUC->0.8604663715858795 Profundidade->15.0 Estimators->250.0\n",
      "AUC: 0.87+-0.02\n"
     ]
    }
   ],
   "source": [
    "x = df_X.loc[:]\n",
    "y = df_y.loc[:]\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "i =0\n",
    "aucs_rf_upselling = []\n",
    "for train,test in cv.split(x,y):    \n",
    "    X_train = x.iloc[train]\n",
    "    X_test = x.iloc[test]\n",
    "    y_train = y.iloc[train]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    X_train = preprocessing.scale(X_train)\n",
    "    X_test = preprocessing.scale(X_test)\n",
    "    \n",
    "    resultados = np.empty([0, 3])\n",
    "    #Testando várias variações dos parametros e pegando o melhor\n",
    "    for n_estimators in range(50,550,50):\n",
    "        for max_depth in range(1,16):\n",
    "            rf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, \n",
    "                                        class_weight='balanced',max_features ='sqrt',random_state = RANDOM_SEED)\n",
    "            pred = rf.fit(X_train,y_train).predict(X_test)\n",
    "            auc = roc_auc_score(y_test,pred)                \n",
    "            saida = np.array([n_estimators,max_depth, auc])            \n",
    "            resultados = np.vstack((resultados, saida))\n",
    "    n_estimators = resultados[np.argmax(resultados[:,2]),0]\n",
    "    max_depth = resultados[np.argmax(resultados[:,2]),1]\n",
    "    auc = resultados[np.argmax(resultados[:,2]),2]   \n",
    "    print(f'{i} fold: AUC->{auc} Profundidade->{max_depth} Estimators->{n_estimators}')    \n",
    "    \n",
    "    aucs_rf_upselling.append(auc)    \n",
    "    i=i+1    \n",
    "\n",
    "AUC_rf_3 = '{}+-{}'.format(round(np.average(aucs_rf_upselling), 2),round(np.std(aucs_rf_upselling),2))\n",
    "print('AUC:', AUC_rf_3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 fold: AUC->0.9136442141623489\n",
      "2 fold: AUC->0.9340457685664939\n",
      "3 fold: AUC->0.9083009499136442\n",
      "4 fold: AUC->0.8844326891935658\n",
      "5 fold: AUC->0.9271294397063586\n",
      "AUC: 0.91+-0.02\n"
     ]
    }
   ],
   "source": [
    "x = df_X.loc[:]\n",
    "y = df_y.loc[:]\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "i =0\n",
    "AUCs_mlp_3 = []\n",
    "for train,test in cv.split(x,y):    \n",
    "    X_train = x.iloc[train]\n",
    "    X_test = x.iloc[test]\n",
    "    y_train = y.iloc[train]\n",
    "    y_test = y.iloc[test]\n",
    "    \n",
    "    X_train = preprocessing.scale(X_train)\n",
    "    X_test = preprocessing.scale(X_test)\n",
    "    \n",
    "    \n",
    "    pred = mlp.fit(X_train,y_train).predict(X_test)\n",
    "    AUCs_mlp_3.append(roc_auc_score(y_test, pred))\n",
    "    i=i+1\n",
    "    print(f'{i} fold: AUC->{roc_auc_score(y_test, pred)}')    \n",
    "\n",
    "AUC_mlp_3 = '{}+-{}'.format(round(np.average(AUCs_mlp_3), 2),round(np.std(AUCs_mlp_3),2))\n",
    "print('AUC:', AUC_mlp_3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melhor resultado utilizando o MLP (0.91+-0.02)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
